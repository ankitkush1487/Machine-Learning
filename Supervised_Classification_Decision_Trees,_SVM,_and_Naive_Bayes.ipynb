{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbYsB99XdBZCYL3kIpfrko",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitkush1487/Machine-Learning/blob/main/Supervised_Classification_Decision_Trees%2C_SVM%2C_and_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "Information Gain measures the reduction in uncertainty (entropy) about a dataset's target variable after splitting it on a particular feature. In decision trees, it is used at each node to select the attribute that provides the most \"information,\" meaning the one that best separates the data into purer subsets, leading to the most effective split. The attribute with the highest information gain is chosen to create the new branches at that node.\n",
        "\n",
        "How it's used in decision trees\n",
        "\n",
        "Calculate entropy: First, the initial uncertainty (entropy) of the entire dataset is calculated.\n",
        "Calculate gain for each feature: For every potential feature to split on, the algorithm calculates the weighted average entropy of the resulting subsets. The information gain is the difference between the original entropy and this new, averaged entropy.\n",
        "Select the best feature: The algorithm selects the feature that yields the highest information gain. This is the feature that reduces the most uncertainty and creates the purest child nodes.\n",
        "Repeat: This process is repeated recursively for each new node until a stopping criterion is met, such as a node becoming pure (e.g., all instances belong to the same class) or a maximum tree depth being reached.\n"
      ],
      "metadata": {
        "id": "ZFrIHi1NWfeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "**Hint: Directly compares the two main impurity measures highlighting strengths, weaknesses, and appropriate use cases.**\n",
        "\n",
        "\n",
        "Definition:\n",
        "\n",
        "Gini Impurity: Measures how often a randomly chosen element would be incorrectly classified if it was randomly labeled according to the class distribution in that node.\n",
        "\n",
        "Entropy: Measures the amount of disorder or uncertainty in the class labels within a node.\n",
        "\n",
        "2Ô∏è‚É£ Formula:\n",
        "\n",
        "Gini:\n",
        "ùê∫\n",
        "ùëñ\n",
        "ùëõ\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini=1‚àí‚àëp\n",
        "i\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Entropy:\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "Entropy=‚àí‚àëp\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "(p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "3Ô∏è‚É£ Range:\n",
        "\n",
        "Gini: Ranges from 0 (pure node) to 0.5 (maximum impurity for two classes).\n",
        "\n",
        "Entropy: Ranges from 0 (pure node) to 1 (maximum impurity for two classes).\n",
        "\n",
        "4Ô∏è‚É£ Computation Speed:\n",
        "\n",
        "Gini: Faster to compute since it doesn‚Äôt use logarithms.\n",
        "\n",
        "Entropy: Slightly slower because it involves log calculations.\n",
        "\n",
        "5Ô∏è‚É£ Interpretation:\n",
        "\n",
        "Gini: Puts more emphasis on the most frequent class; it prefers larger class separation.\n",
        "\n",
        "Entropy: Considers the overall information gain and is more sensitive to small changes in class probabilities.\n",
        "\n",
        "6Ô∏è‚É£ Common Use in Algorithms:\n",
        "\n",
        "Gini: Used by CART (Classification and Regression Trees).\n",
        "\n",
        "Entropy: Used by ID3 and C4.5 decision tree algorithms.\n",
        "\n",
        "7Ô∏è‚É£ When to Use:\n",
        "\n",
        "Gini: Prefer when speed and simplicity are priorities.\n",
        "\n",
        "Entropy: Prefer when you want a more information-theoretic and detailed measure of impurity."
      ],
      "metadata": {
        "id": "P2K8sWCyWxoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "Pre-pruning, or early stopping, is a technique in decision trees where the growth of the tree is halted before it becomes fully developed, preventing overfitting by using criteria to stop splitting nodes. Common stopping conditions include reaching a maximum depth, not having enough samples in a node, or a split not improving the model's impurity enough."
      ],
      "metadata": {
        "id": "GP_J1uDrXXh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).**\n",
        "\n",
        "**Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.**\n",
        "**(Include your Python code and output in the code box below.)**\n"
      ],
      "metadata": {
        "id": "GklT5v1JXqgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset as an example\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini impurity as the criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"{feature_names[i]}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGIo28lrX82C",
        "outputId": "efa9ae39-6703-472b-8e3f-a6c35cad7319"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "A Support Vector Machine (SVM) is a powerful, supervised machine learning algorithm used for both classification and regression tasks. Its primary goal is to find the optimal decision boundary, known as a hyperplane, that maximally separates data points of different classes in a high-dimensional space\n",
        "\n",
        "\n",
        "Hyperplane: The decision boundary separating data points of different classes. This is a line in 2D or a plane in higher dimensions.\n",
        "\n",
        "Margin: The distance between the hyperplane and the closest data points from each class. The algorithm aims to maximize this distance for better performance.\n",
        "\n",
        "Support Vectors: The data points closest to the hyperplane that determine its position.\n",
        "\n",
        "Kernel Trick: A method using kernel functions to map non-linearly separable data into a higher-dimensional space where a linear separation is possible without explicit transformation"
      ],
      "metadata": {
        "id": "Cb_grlv4YEOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: What is the Kernel Trick in SVM?**\n",
        "\n",
        "The Kernel Trick is a method used in Support Vector Machines (SVMs) that allows for the classification of non-linear data by implicitly mapping it to a higher-dimensional space. Instead of explicitly transforming the data, the trick uses a kernel function to compute the dot product of the data points in that higher-dimensional space, which is computationally efficient. This allows the algorithm to find a linear separator in the new, higher-dimensional space that corresponds to a non-linear decision boundary in the original space"
      ],
      "metadata": {
        "id": "z4XFnnXXYXFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**\n",
        "\n",
        "**Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.**\n",
        "**(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "GJQFRS54Yh5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into 70% training and 30% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"SVM with Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"SVM with RBF Kernel Accuracy:\", acc_rbf)\n",
        "\n",
        "# Compare which performed better\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"\\nLinear Kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"\\nRBF Kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\nBoth kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yzl5OhMJY4GC",
        "outputId": "27125a0c-1e98-41c5-c789-26f72faa7f19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM with Linear Kernel Accuracy: 0.9629629629629629\n",
            "SVM with RBF Kernel Accuracy: 0.9814814814814815\n",
            "\n",
            "RBF Kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?**\n",
        "\n",
        "The Na√Øve Bayes classifier is a probability-based machine learning algorithm that uses Bayes' Theorem to classify data points. It is called \"na√Øve\" because it makes a strong and often unrealistic assumption that all features are independent of each other, meaning the presence of one feature does not affect the others.\n",
        "\n",
        "What it is\n",
        "A supervised classification algorithm: It is used for both binary and multi-class classification problems.\n",
        "Based on Bayes' Theorem: The algorithm uses this theorem to calculate the probability of a class given a set of features. It predicts the class with the highest probability.\n",
        "Effective in practice: Despite its simplistic assumption, it performs well in many real-world applications, such as spam filtering and text categorization.\n",
        "\n",
        "Why it is called \"na√Øve\"\n",
        "\n",
        "The \"na√Øve\" assumption: The core assumption is that all features used for classification are independent of each other.\n",
        "Unrealistic in real-world data: This is often not true in reality, as features are frequently correlated. For example, in a text document, the presence of one word might make another word more likely to be present.\n",
        "\n",
        "Simplifies computation: The independence assumption simplifies the complex probability calculations, making the algorithm easier and faster to compute, even though it is an oversimplification."
      ],
      "metadata": {
        "id": "GzBklI8qZE5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes**\n",
        "\n",
        "1Ô∏è‚É£ Gaussian Na√Øve Bayes\n",
        "\n",
        "Used for: Continuous (real-valued) features\n",
        "Assumption: Each feature follows a normal (Gaussian) distribution within each class.\n",
        "\n",
        "Example use case:\n",
        "\n",
        "Predicting based on continuous data like height, weight, or age.\n",
        "\n",
        "Commonly used in iris classification, medical diagnosis, etc.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùë¶\n",
        ")=1\n",
        "2\n",
        "ùúã\n",
        "ùúé\n",
        "ùë¶\n",
        "2\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùúá\n",
        "ùë¶\n",
        ")\n",
        "2\n",
        "2\n",
        "ùúé\n",
        "ùë¶\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚à£y)=\n",
        "2œÄœÉ\n",
        "y\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "exp(‚àí\n",
        "2œÉ\n",
        "y\n",
        "2\n",
        "(x\n",
        "i\n",
        "‚àíŒº\n",
        "y\n",
        ")\n",
        "2\t‚Äã\n",
        ")\n",
        "\n",
        "Key idea:\n",
        "Uses the mean (Œº) and variance (œÉ¬≤) of each feature for each class.\n",
        "\n",
        "2Ô∏è‚É£ Multinomial Na√Øve Bayes\n",
        "\n",
        "Used for: Discrete or count-based features\n",
        "Assumption: Features represent frequency or count data (non-negative integers).\n",
        "\n",
        "Example use case:\n",
        "\n",
        "Text classification (spam detection, sentiment analysis).\n",
        "\n",
        "Word counts or TF-IDF scores in documents.\n",
        "\n",
        "Key idea:\n",
        "Estimates the probability of a feature (e.g., a word) occurring within a class, based on counts.\n",
        "\n",
        "3Ô∏è‚É£ Bernoulli Na√Øve Bayes\n",
        "\n",
        "Used for: Binary/Boolean features (0 or 1)\n",
        "Assumption: Each feature represents a yes/no or present/absent condition.\n",
        "\n",
        "Example use case:\n",
        "\n",
        "Text classification where only the presence or absence of a word matters (not the count).\n",
        "\n",
        "Suitable for binary feature vectors."
      ],
      "metadata": {
        "id": "EsQ0ewdkZa3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Breast Cancer Dataset**\n",
        "**Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer**\n",
        "**dataset and evaluate accuracy.**\n",
        "\n",
        "**Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from**\n",
        "**sklearn.datasets.**\n",
        "**(Include your Python code and output in the code box below.)**\n"
      ],
      "metadata": {
        "id": "ZzKG3bs0aPpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data  # Features\n",
        "y = breast_cancer.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "# We use a fixed random_state for reproducibility of results\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of samples in the dataset: {len(X)}\")\n",
        "print(f\"Number of training samples: {len(X_train)}\")\n",
        "print(f\"Number of testing samples: {len(X_test)}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Accuracy of the Gaussian Naive Bayes classifier: {accuracy:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3SYD87Aah47",
        "outputId": "2abc42c6-94bc-4ea5-c4a6-6c6002ee2382"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the dataset: 569\n",
            "Number of training samples: 455\n",
            "Number of testing samples: 114\n",
            "------------------------------\n",
            "Accuracy of the Gaussian Naive Bayes classifier: 0.9737\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}